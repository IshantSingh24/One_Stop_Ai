{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe37963-1af6-44fc-a841-8e462443f5e6",
   "metadata": {},
   "source": [
    "## Expert Knowledge Worker\n",
    "\n",
    "### A question answering agent that is an expert knowledge worker\n",
    "### To be used by employees of Insurellm, an Insurance Tech company\n",
    "### The agent needs to be accurate and the solution should be low cost.\n",
    "\n",
    "This project will use RAG (Retrieval Augmented Generation) to ensure our question/answering assistant has high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d7d1c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in ./.venv/lib/python3.12/site-packages (1.2.0)\n",
      "Requirement already satisfied: python-pptx in ./.venv/lib/python3.12/site-packages (1.0.2)\n",
      "Requirement already satisfied: lxml>=3.1.0 in ./.venv/lib/python3.12/site-packages (from python-docx) (6.0.1)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in ./.venv/lib/python3.12/site-packages (from python-docx) (4.15.0)\n",
      "Requirement already satisfied: Pillow>=3.3.2 in ./.venv/lib/python3.12/site-packages (from python-pptx) (11.3.0)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in ./.venv/lib/python3.12/site-packages (from python-pptx) (3.2.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-docx python-pptx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba2779af-84ef-4227-9e9e-6eaf0df87e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "802137aa-8a74-45e0-a487-d1974927d7ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.schema import Document\n",
    "import docx\n",
    "import pptx\n",
    "import json\n",
    "\n",
    "def load_documents(path=\"knowledge_base\"):\n",
    "    documents = []\n",
    "    for root, _, files in os.walk(path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            content = \"\"\n",
    "            try:\n",
    "                if file.endswith(\".md\"):\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        content = f.read()\n",
    "                elif file.endswith(\".txt\"):\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        content = f.read()\n",
    "                elif file.endswith(\".docx\"):\n",
    "                    doc = docx.Document(file_path)\n",
    "                    content = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "                elif file.endswith(\".pptx\"):\n",
    "                    pres = pptx.Presentation(file_path)\n",
    "                    for slide in pres.slides:\n",
    "                        for shape in slide.shapes:\n",
    "                            if hasattr(shape, \"text\"):\n",
    "                                content += shape.text + \"\\n\"\n",
    "                elif file.endswith(\".json\"):\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        data = json.load(f)\n",
    "                        content = json.dumps(data, indent=2)\n",
    "                \n",
    "                if content:\n",
    "                    # Get the parent directory name as the doc_type\n",
    "                    doc_type = os.path.basename(root)\n",
    "                    documents.append(Document(page_content=content, metadata={\"source\": file_path, \"doc_type\": doc_type}))\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading file {file_path}: {e}\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58c85082-e417-4708-9efe-81a5d55d1424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# price is a factor for our company, so we're going to use a low cost model\n",
    "\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "db_name = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee78efcb-60fe-449e-a944-40bab26261af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv(override=True)\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "730711a9-6ffe-4eee-8f48-d6cfb7314905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4 documents.\n"
     ]
    }
   ],
   "source": [
    "documents = load_documents()\n",
    "print(f\"Loaded {len(documents)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7310c9c8-03c1-4efc-a104-5e89aec6db1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 documents, with 4 being valid after filtering.\n"
     ]
    }
   ],
   "source": [
    "# Filter out documents with no content before splitting\n",
    "valid_documents = [doc for doc in documents if doc.page_content and doc.page_content.strip()]\n",
    "print(f\"Found {len(documents)} documents, with {len(valid_documents)} being valid after filtering.\")\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(valid_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd06e02f-6d9b-44cc-a43d-e1faa8acc7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c54b4b6-06da-463d-bee7-4dd456c2b887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document types found: drive, slack\n"
     ]
    }
   ],
   "source": [
    "doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)\n",
    "print(f\"Document types found: {', '.join(doc_types)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f7d2a6-ccfa-425b-a1c3-5e55b23bd013",
   "metadata": {},
   "source": [
    "## A sidenote on Embeddings, and \"Auto-Encoding LLMs\"\n",
    "\n",
    "We will be mapping each chunk of text into a Vector that represents the meaning of the text, known as an embedding.\n",
    "\n",
    "OpenAI offers a model to do this, which we will use by calling their API with some LangChain code.\n",
    "\n",
    "This model is an example of an \"Auto-Encoding LLM\" which generates an output given a complete input.\n",
    "It's different to all the other LLMs we've discussed today, which are known as \"Auto-Regressive LLMs\", and generate future tokens based only on past context.\n",
    "\n",
    "Another example of an Auto-Encoding LLMs is BERT from Google. In addition to embedding, Auto-encoding LLMs are often used for classification.\n",
    "\n",
    "### Sidenote\n",
    "\n",
    "In week 8 we will return to RAG and vector embeddings, and we will use an open-source vector encoder so that the data never leaves our computer - that's an important consideration when building enterprise systems and the data needs to remain internal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78998399-ac17-4e28-b15f-0b5f51e6ee23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_61083/4068487649.py:19: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to existing vectorstore with 55 documents.\n"
     ]
    }
   ],
   "source": [
    "# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk\n",
    "# Chroma is a popular open source Vector Database based on SQLLite\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "collection_name = \"insurellm_docs\" # Use a consistent collection name\n",
    "\n",
    "# If you would rather use the free Vector Embeddings from HuggingFace sentence-transformers\n",
    "# Then replace embeddings = OpenAIEmbeddings()\n",
    "# with:\n",
    "# from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# This notebook is designed to build the DB from scratch.\n",
    "# The watchdog script (`untitled.py`) is designed to keep it updated.\n",
    "# For the live-update to work, we should not delete the collection every time.\n",
    "# We will connect to it, and if it's empty, we will populate it.\n",
    "\n",
    "# Connect to the vectorstore\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=db_name, \n",
    "    embedding_function=embeddings,\n",
    "    collection_name=collection_name\n",
    ")\n",
    "\n",
    "# If the database is empty, populate it from the documents loaded earlier.\n",
    "if vectorstore._collection.count() == 0:\n",
    "    print(\"Database is empty. Populating with initial documents...\")\n",
    "    vectorstore.add_documents(chunks)\n",
    "    print(f\"Vectorstore populated with {vectorstore._collection.count()} documents.\")\n",
    "else:\n",
    "    print(f\"Connected to existing vectorstore with {vectorstore._collection.count()} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "057868f6-51a6-4087-94d1-380145821550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vectors have 1,536 dimensions\n"
     ]
    }
   ],
   "source": [
    "# Get one vector and find how many dimensions it has\n",
    "\n",
    "collection = vectorstore._collection\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"The vectors have {dimensions:,} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ac69fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever from the vector store\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Create a ChatOpenAI model\n",
    "llm = ChatOpenAI(model_name=MODEL, temperature=0)\n",
    "\n",
    "# Create a RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d45462-a818-441c-b010-b85b32bcf618",
   "metadata": {},
   "source": [
    "## Visualizing the Vector Store\n",
    "\n",
    "Let's take a minute to look at the documents and their embedding vectors to see what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1631ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in ./.venv/lib/python3.12/site-packages (5.44.1)\n",
      "Requirement already satisfied: watchdog in ./.venv/lib/python3.12/site-packages (6.0.0)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in ./.venv/lib/python3.12/site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in ./.venv/lib/python3.12/site-packages (from gradio) (4.10.0)\n",
      "Requirement already satisfied: brotli>=1.1.0 in ./.venv/lib/python3.12/site-packages (from gradio) (1.1.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in ./.venv/lib/python3.12/site-packages (from gradio) (0.116.1)\n",
      "Requirement already satisfied: ffmpy in ./.venv/lib/python3.12/site-packages (from gradio) (0.6.1)\n",
      "Requirement already satisfied: gradio-client==1.12.1 in ./.venv/lib/python3.12/site-packages (from gradio) (1.12.1)\n",
      "Requirement already satisfied: groovy~=0.1 in ./.venv/lib/python3.12/site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in ./.venv/lib/python3.12/site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.33.5 in ./.venv/lib/python3.12/site-packages (from gradio) (0.34.4)\n",
      "Requirement already satisfied: jinja2<4.0 in ./.venv/lib/python3.12/site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in ./.venv/lib/python3.12/site-packages (from gradio) (3.0.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in ./.venv/lib/python3.12/site-packages (from gradio) (2.3.2)\n",
      "Requirement already satisfied: orjson~=3.0 in ./.venv/lib/python3.12/site-packages (from gradio) (3.11.3)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.12/site-packages (from gradio) (25.0)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in ./.venv/lib/python3.12/site-packages (from gradio) (2.3.2)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in ./.venv/lib/python3.12/site-packages (from gradio) (11.3.0)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in ./.venv/lib/python3.12/site-packages (from gradio) (2.11.7)\n",
      "Requirement already satisfied: pydub in ./.venv/lib/python3.12/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in ./.venv/lib/python3.12/site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in ./.venv/lib/python3.12/site-packages (from gradio) (6.0.2)\n",
      "Requirement already satisfied: ruff>=0.9.3 in ./.venv/lib/python3.12/site-packages (from gradio) (0.12.11)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in ./.venv/lib/python3.12/site-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in ./.venv/lib/python3.12/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in ./.venv/lib/python3.12/site-packages (from gradio) (0.47.3)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in ./.venv/lib/python3.12/site-packages (from gradio) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in ./.venv/lib/python3.12/site-packages (from gradio) (0.16.1)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in ./.venv/lib/python3.12/site-packages (from gradio) (4.15.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in ./.venv/lib/python3.12/site-packages (from gradio) (0.35.0)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from gradio-client==1.12.1->gradio) (2025.7.0)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in ./.venv/lib/python3.12/site-packages (from gradio-client==1.12.1->gradio) (15.0.1)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.12/site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.12/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.12/site-packages (from httpx<1.0,>=0.24.1->gradio) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (3.19.1)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (1.1.9)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.12/site-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.12/site-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
      "Requirement already satisfied: click>=8.0.0 in ./.venv/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./.venv/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./.venv/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (14.1.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (2.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio watchdog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26c170bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/variable/One_Stop_Ai/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_61083/4014041867.py:105: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  iface = gr.ChatInterface(fn=chat, title=\"Knowledge Base Chat\", chatbot=gr.Chatbot(height=600))\n",
      "/home/variable/One_Stop_Ai/.venv/lib/python3.12/site-packages/gradio/chat_interface.py:328: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watching for changes in knowledge_base\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "import threading\n",
    "from watchdog.observers import Observer\n",
    "from watchdog.events import FileSystemEventHandler\n",
    "\n",
    "# Global flag to signal when the knowledge base has been updated\n",
    "update_notification = \"\"\n",
    "\n",
    "def load_single_document(file_path):\n",
    "    \"\"\"Loads a single document from a file path.\"\"\"\n",
    "    content = \"\"\n",
    "    doc_type = os.path.basename(os.path.dirname(file_path))\n",
    "    try:\n",
    "        if file_path.endswith((\".md\", \".txt\")):\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read()\n",
    "        elif file_path.endswith(\".docx\"):\n",
    "            import docx\n",
    "            doc = docx.Document(file_path)\n",
    "            content = \"\\n\".join([p.text for p in doc.paragraphs])\n",
    "        elif file_path.endswith(\".pptx\"):\n",
    "            import pptx\n",
    "            pres = pptx.Presentation(file_path)\n",
    "            content = \"\\n\".join(\n",
    "                shape.text for slide in pres.slides for shape in slide.shapes if hasattr(shape, \"text\")\n",
    "            )\n",
    "        elif file_path.endswith(\".json\"):\n",
    "            import json\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "                content = json.dumps(data, indent=2)\n",
    "        \n",
    "        if content:\n",
    "            return [Document(page_content=content, metadata={\"source\": file_path, \"doc_type\": doc_type})]\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file {file_path}: {e}\")\n",
    "    return None\n",
    "\n",
    "class KnowledgeBaseWatcher(FileSystemEventHandler):\n",
    "    def __init__(self, vectorstore, text_splitter):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.text_splitter = text_splitter\n",
    "\n",
    "    def on_modified(self, event):\n",
    "        if not event.is_directory:\n",
    "            self.update_vectorstore(event.src_path)\n",
    "\n",
    "    def on_created(self, event):\n",
    "        if not event.is_directory:\n",
    "            self.update_vectorstore(event.src_path)\n",
    "\n",
    "    def update_vectorstore(self, file_path):\n",
    "        global update_notification\n",
    "        print(f\"Detected change in {file_path}. Updating vector store.\")\n",
    "        \n",
    "        # Delete existing documents from the vector store that match the source file\n",
    "        existing_docs = self.vectorstore.get(where={'source': file_path})\n",
    "        if existing_docs['ids']:\n",
    "            self.vectorstore.delete(ids=existing_docs['ids'])\n",
    "            print(f\"Deleted {len(existing_docs['ids'])} old chunks for {file_path}\")\n",
    "\n",
    "        # Load the new/modified document and add it to the vector store\n",
    "        docs = load_single_document(file_path)\n",
    "        if docs:\n",
    "            chunks = self.text_splitter.split_documents(docs)\n",
    "            self.vectorstore.add_documents(chunks)\n",
    "            print(f\"Added {len(chunks)} new chunks for {file_path}\")\n",
    "            update_notification = f\"Knowledge base updated with changes from: {os.path.basename(file_path)}\"\n",
    "\n",
    "def start_watcher(path, vectorstore, text_splitter):\n",
    "    event_handler = KnowledgeBaseWatcher(vectorstore, text_splitter)\n",
    "    observer = Observer()\n",
    "    observer.schedule(event_handler, path, recursive=True)\n",
    "    observer.start()\n",
    "    print(f\"Watching for changes in {path}\")\n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        observer.stop()\n",
    "    observer.join()\n",
    "\n",
    "# Start the file watcher in a background thread\n",
    "watcher_thread = threading.Thread(\n",
    "    target=start_watcher,\n",
    "    args=(\"knowledge_base\", vectorstore, text_splitter),\n",
    "    daemon=True\n",
    ")\n",
    "watcher_thread.start()\n",
    "\n",
    "def chat(message, history):\n",
    "    global update_notification\n",
    "    \n",
    "    # Check if there's an update notification to display\n",
    "    if update_notification:\n",
    "        notification_message = f\"âœ¨ *Knowledge base was recently updated. The answer below may reflect new information.*\\n\\n---\\n\"\n",
    "        update_notification = \"\" # Reset after displaying\n",
    "    else:\n",
    "        notification_message = \"\"\n",
    "\n",
    "    response = qa_chain.invoke({\"query\": message})\n",
    "    return notification_message + response[\"result\"]\n",
    "\n",
    "iface = gr.ChatInterface(fn=chat, title=\"Knowledge Base Chat\", chatbot=gr.Chatbot(height=600))\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f08c5cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/thunder/projects/one_stop/knowledge_base/drive/ai_team_members.docx'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Save the document\u001b[39;00m\n\u001b[32m     22\u001b[39m file_path = \u001b[33m\"\u001b[39m\u001b[33m/home/thunder/projects/one_stop/knowledge_base/drive/ai_team_members.docx\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mdoc\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCreated dummy docx file at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/One_Stop_Ai/.venv/lib/python3.12/site-packages/docx/document.py:204\u001b[39m, in \u001b[36mDocument.save\u001b[39m\u001b[34m(self, path_or_stream)\u001b[39m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave\u001b[39m(\u001b[38;5;28mself\u001b[39m, path_or_stream: \u001b[38;5;28mstr\u001b[39m | IO[\u001b[38;5;28mbytes\u001b[39m]):\n\u001b[32m    199\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Save this document to `path_or_stream`.\u001b[39;00m\n\u001b[32m    200\u001b[39m \n\u001b[32m    201\u001b[39m \u001b[33;03m    `path_or_stream` can be either a path to a filesystem location (a string) or a\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[33;03m    file-like object.\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_part\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_stream\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/One_Stop_Ai/.venv/lib/python3.12/site-packages/docx/parts/document.py:114\u001b[39m, in \u001b[36mDocumentPart.save\u001b[39m\u001b[34m(self, path_or_stream)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave\u001b[39m(\u001b[38;5;28mself\u001b[39m, path_or_stream: \u001b[38;5;28mstr\u001b[39m | IO[\u001b[38;5;28mbytes\u001b[39m]):\n\u001b[32m    112\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Save this document to `path_or_stream`, which can be either a path to a\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m    filesystem location (a string) or a file-like object.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_stream\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/One_Stop_Ai/.venv/lib/python3.12/site-packages/docx/opc/package.py:166\u001b[39m, in \u001b[36mOpcPackage.save\u001b[39m\u001b[34m(self, pkg_file)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.parts:\n\u001b[32m    165\u001b[39m     part.before_marshal()\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m \u001b[43mPackageWriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpkg_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparts\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/One_Stop_Ai/.venv/lib/python3.12/site-packages/docx/opc/pkgwriter.py:34\u001b[39m, in \u001b[36mPackageWriter.write\u001b[39m\u001b[34m(pkg_file, pkg_rels, parts)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrite\u001b[39m(pkg_file, pkg_rels, parts):\n\u001b[32m     32\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Write a physical package (.pptx file) to `pkg_file` containing `pkg_rels` and\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[33;03m    `parts` and a content types stream based on the content types of the parts.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     phys_writer = \u001b[43mPhysPkgWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpkg_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m     PackageWriter._write_content_types_stream(phys_writer, parts)\n\u001b[32m     36\u001b[39m     PackageWriter._write_pkg_rels(phys_writer, pkg_rels)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/One_Stop_Ai/.venv/lib/python3.12/site-packages/docx/opc/phys_pkg.py:109\u001b[39m, in \u001b[36m_ZipPkgWriter.__init__\u001b[39m\u001b[34m(self, pkg_file)\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pkg_file):\n\u001b[32m    108\u001b[39m     \u001b[38;5;28msuper\u001b[39m(_ZipPkgWriter, \u001b[38;5;28mself\u001b[39m).\u001b[34m__init__\u001b[39m()\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     \u001b[38;5;28mself\u001b[39m._zipf = \u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpkg_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mw\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mZIP_DEFLATED\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/zipfile/__init__.py:1331\u001b[39m, in \u001b[36mZipFile.__init__\u001b[39m\u001b[34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[39m\n\u001b[32m   1329\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m   1330\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1331\u001b[39m         \u001b[38;5;28mself\u001b[39m.fp = \u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilemode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1332\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[32m   1333\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m filemode \u001b[38;5;129;01min\u001b[39;00m modeDict:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/home/thunder/projects/one_stop/knowledge_base/drive/ai_team_members.docx'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1343, which is longer than the specified 1000\n",
      "Created a chunk of size 1246, which is longer than the specified 1000\n",
      "Created a chunk of size 1812, which is longer than the specified 1000\n",
      "Created a chunk of size 4615, which is longer than the specified 1000\n",
      "Created a chunk of size 24443, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected change in knowledge_base/slack/20250829_235312_CREATE DEFINER user TRIGGER.txt. Updating vector store.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1343, which is longer than the specified 1000\n",
      "Created a chunk of size 1246, which is longer than the specified 1000\n",
      "Created a chunk of size 1812, which is longer than the specified 1000\n",
      "Created a chunk of size 4615, which is longer than the specified 1000\n",
      "Created a chunk of size 24443, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 25 new chunks for knowledge_base/slack/20250829_235312_CREATE DEFINER user TRIGGER.txt\n",
      "Detected change in knowledge_base/slack/20250829_235312_CREATE DEFINER user TRIGGER.txt. Updating vector store.\n",
      "Deleted 25 old chunks for knowledge_base/slack/20250829_235312_CREATE DEFINER user TRIGGER.txt\n",
      "Added 25 new chunks for knowledge_base/slack/20250829_235312_CREATE DEFINER user TRIGGER.txt\n"
     ]
    }
   ],
   "source": [
    "import docx\n",
    "\n",
    "# Create a new Document\n",
    "doc = docx.Document()\n",
    "\n",
    "# Add a heading\n",
    "doc.add_heading('AI Team Members', level=1)\n",
    "\n",
    "# Add team member names\n",
    "team_members = [\n",
    "    \"Alice - Lead AI Scientist\",\n",
    "    \"Bob - Machine Learning Engineer\",\n",
    "    \"Charlie - Data Scientist\",\n",
    "    \"David - AI Ethicist\",\n",
    "    \"Eve - NLP Specialist\"\n",
    "]\n",
    "\n",
    "for member in team_members:\n",
    "    doc.add_paragraph(member, style='List Bullet')\n",
    "\n",
    "# Save the document\n",
    "file_path = \"/home/thunder/projects/one_stop/knowledge_base/drive/ai_team_members.docx\"\n",
    "doc.save(file_path)\n",
    "\n",
    "print(f\"Created dummy docx file at: {file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
