{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe37963-1af6-44fc-a841-8e462443f5e6",
   "metadata": {},
   "source": [
    "## Expert Knowledge Worker\n",
    "\n",
    "### A question answering agent that is an expert knowledge worker\n",
    "### To be used by employees of Insurellm, an Insurance Tech company\n",
    "### The agent needs to be accurate and the solution should be low cost.\n",
    "\n",
    "This project will use RAG (Retrieval Augmented Generation) to ensure our question/answering assistant has high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d7d1c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (1.2.0)\n",
      "Requirement already satisfied: python-pptx in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (1.0.2)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from python-docx) (6.0.1)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from python-docx) (4.14.1)\n",
      "Requirement already satisfied: Pillow>=3.3.2 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from python-pptx) (11.3.0)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from python-pptx) (3.2.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-docx python-pptx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba2779af-84ef-4227-9e9e-6eaf0df87e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "802137aa-8a74-45e0-a487-d1974927d7ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.schema import Document\n",
    "import docx\n",
    "import pptx\n",
    "import json\n",
    "\n",
    "def load_documents(path=\"knowledge_base\"):\n",
    "    documents = []\n",
    "    for root, _, files in os.walk(path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            content = \"\"\n",
    "            try:\n",
    "                if file.endswith(\".md\"):\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        content = f.read()\n",
    "                elif file.endswith(\".txt\"):\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        content = f.read()\n",
    "                elif file.endswith(\".docx\"):\n",
    "                    doc = docx.Document(file_path)\n",
    "                    content = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "                elif file.endswith(\".pptx\"):\n",
    "                    pres = pptx.Presentation(file_path)\n",
    "                    for slide in pres.slides:\n",
    "                        for shape in slide.shapes:\n",
    "                            if hasattr(shape, \"text\"):\n",
    "                                content += shape.text + \"\\n\"\n",
    "                elif file.endswith(\".json\"):\n",
    "                    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                        data = json.load(f)\n",
    "                        content = json.dumps(data, indent=2)\n",
    "                \n",
    "                if content:\n",
    "                    # Get the parent directory name as the doc_type\n",
    "                    doc_type = os.path.basename(root)\n",
    "                    documents.append(Document(page_content=content, metadata={\"source\": file_path, \"doc_type\": doc_type}))\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading file {file_path}: {e}\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58c85082-e417-4708-9efe-81a5d55d1424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# price is a factor for our company, so we're going to use a low cost model\n",
    "\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "db_name = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee78efcb-60fe-449e-a944-40bab26261af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv(override=True)\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "730711a9-6ffe-4eee-8f48-d6cfb7314905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 documents.\n"
     ]
    }
   ],
   "source": [
    "documents = load_documents()\n",
    "print(f\"Loaded {len(documents)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7310c9c8-03c1-4efc-a104-5e89aec6db1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 documents, with 3 being valid after filtering.\n"
     ]
    }
   ],
   "source": [
    "# Filter out documents with no content before splitting\n",
    "valid_documents = [doc for doc in documents if doc.page_content and doc.page_content.strip()]\n",
    "print(f\"Found {len(documents)} documents, with {len(valid_documents)} being valid after filtering.\")\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(valid_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd06e02f-6d9b-44cc-a43d-e1faa8acc7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c54b4b6-06da-463d-bee7-4dd456c2b887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document types found: slack, knowledge_base, drive\n"
     ]
    }
   ],
   "source": [
    "doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)\n",
    "print(f\"Document types found: {', '.join(doc_types)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f7d2a6-ccfa-425b-a1c3-5e55b23bd013",
   "metadata": {},
   "source": [
    "## A sidenote on Embeddings, and \"Auto-Encoding LLMs\"\n",
    "\n",
    "We will be mapping each chunk of text into a Vector that represents the meaning of the text, known as an embedding.\n",
    "\n",
    "OpenAI offers a model to do this, which we will use by calling their API with some LangChain code.\n",
    "\n",
    "This model is an example of an \"Auto-Encoding LLM\" which generates an output given a complete input.\n",
    "It's different to all the other LLMs we've discussed today, which are known as \"Auto-Regressive LLMs\", and generate future tokens based only on past context.\n",
    "\n",
    "Another example of an Auto-Encoding LLMs is BERT from Google. In addition to embedding, Auto-encoding LLMs are often used for classification.\n",
    "\n",
    "### Sidenote\n",
    "\n",
    "In week 8 we will return to RAG and vector embeddings, and we will use an open-source vector encoder so that the data never leaves our computer - that's an important consideration when building enterprise systems and the data needs to remain internal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78998399-ac17-4e28-b15f-0b5f51e6ee23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to existing vectorstore with 3 documents.\n"
     ]
    }
   ],
   "source": [
    "# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk\n",
    "# Chroma is a popular open source Vector Database based on SQLLite\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "collection_name = \"insurellm_docs\" # Use a consistent collection name\n",
    "\n",
    "# If you would rather use the free Vector Embeddings from HuggingFace sentence-transformers\n",
    "# Then replace embeddings = OpenAIEmbeddings()\n",
    "# with:\n",
    "# from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# This notebook is designed to build the DB from scratch.\n",
    "# The watchdog script (`untitled.py`) is designed to keep it updated.\n",
    "# For the live-update to work, we should not delete the collection every time.\n",
    "# We will connect to it, and if it's empty, we will populate it.\n",
    "\n",
    "# Connect to the vectorstore\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=db_name, \n",
    "    embedding_function=embeddings,\n",
    "    collection_name=collection_name\n",
    ")\n",
    "\n",
    "# If the database is empty, populate it from the documents loaded earlier.\n",
    "if vectorstore._collection.count() == 0:\n",
    "    print(\"Database is empty. Populating with initial documents...\")\n",
    "    vectorstore.add_documents(chunks)\n",
    "    print(f\"Vectorstore populated with {vectorstore._collection.count()} documents.\")\n",
    "else:\n",
    "    print(f\"Connected to existing vectorstore with {vectorstore._collection.count()} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "057868f6-51a6-4087-94d1-380145821550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vectors have 1,536 dimensions\n"
     ]
    }
   ],
   "source": [
    "# Get one vector and find how many dimensions it has\n",
    "\n",
    "collection = vectorstore._collection\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"The vectors have {dimensions:,} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ac69fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever from the vector store\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Create a ChatOpenAI model\n",
    "llm = ChatOpenAI(model_name=MODEL, temperature=0)\n",
    "\n",
    "# Create a RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d45462-a818-441c-b010-b85b32bcf618",
   "metadata": {},
   "source": [
    "## Visualizing the Vector Store\n",
    "\n",
    "Let's take a minute to look at the documents and their embedding vectors to see what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1631ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (5.43.1)\n",
      "Requirement already satisfied: watchdog in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (6.0.0)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from gradio) (4.10.0)\n",
      "Requirement already satisfied: brotli>=1.1.0 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from gradio) (1.1.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from gradio) (0.115.9)\n",
      "Requirement already satisfied: ffmpy in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from gradio) (0.6.1)\n",
      "Requirement already satisfied: gradio-client==1.12.1 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from gradio) (1.12.1)\n",
      "Requirement already satisfied: groovy~=0.1 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.33.5 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from gradio) (0.34.4)\n",
      "Requirement already satisfied: jinja2<4.0 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from gradio) (3.0.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from gradio) (1.26.4)\n",
      "Requirement already satisfied: orjson~=3.0 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from gradio) (3.11.2)\n",
      "Requirement already satisfied: packaging in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from gradio) (25.0)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from gradio) (2.3.2)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from gradio) (11.3.0)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from gradio) (2.11.7)\n",
      "Requirement already satisfied: pydub in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from gradio) (6.0.2)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from gradio) (0.12.10)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from gradio) (0.45.3)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from gradio) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from gradio) (0.16.1)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from gradio) (4.14.1)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from gradio) (0.35.0)\n",
      "Requirement already satisfied: fsspec in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from gradio-client==1.12.1->gradio) (2025.3.0)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from gradio-client==1.12.1->gradio) (15.0.1)\n",
      "Requirement already satisfied: idna>=2.8 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: certifi in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from httpx<1.0,>=0.24.1->gradio) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: filelock in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (3.19.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (1.1.8)\n",
      "Requirement already satisfied: requests in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from typer<1.0,>=0.12->gradio) (14.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from typer<1.0,>=0.12->gradio) (14.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (2.5.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (2.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio watchdog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26c170bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watching for changes in knowledge_base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8821/4014041867.py:105: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  iface = gr.ChatInterface(fn=chat, title=\"Knowledge Base Chat\", chatbot=gr.Chatbot(height=600))\n",
      "/home/thunder/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/chat_interface.py:328: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "import threading\n",
    "from watchdog.observers import Observer\n",
    "from watchdog.events import FileSystemEventHandler\n",
    "\n",
    "# Global flag to signal when the knowledge base has been updated\n",
    "update_notification = \"\"\n",
    "\n",
    "def load_single_document(file_path):\n",
    "    \"\"\"Loads a single document from a file path.\"\"\"\n",
    "    content = \"\"\n",
    "    doc_type = os.path.basename(os.path.dirname(file_path))\n",
    "    try:\n",
    "        if file_path.endswith((\".md\", \".txt\")):\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read()\n",
    "        elif file_path.endswith(\".docx\"):\n",
    "            import docx\n",
    "            doc = docx.Document(file_path)\n",
    "            content = \"\\n\".join([p.text for p in doc.paragraphs])\n",
    "        elif file_path.endswith(\".pptx\"):\n",
    "            import pptx\n",
    "            pres = pptx.Presentation(file_path)\n",
    "            content = \"\\n\".join(\n",
    "                shape.text for slide in pres.slides for shape in slide.shapes if hasattr(shape, \"text\")\n",
    "            )\n",
    "        elif file_path.endswith(\".json\"):\n",
    "            import json\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "                content = json.dumps(data, indent=2)\n",
    "        \n",
    "        if content:\n",
    "            return [Document(page_content=content, metadata={\"source\": file_path, \"doc_type\": doc_type})]\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file {file_path}: {e}\")\n",
    "    return None\n",
    "\n",
    "class KnowledgeBaseWatcher(FileSystemEventHandler):\n",
    "    def __init__(self, vectorstore, text_splitter):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.text_splitter = text_splitter\n",
    "\n",
    "    def on_modified(self, event):\n",
    "        if not event.is_directory:\n",
    "            self.update_vectorstore(event.src_path)\n",
    "\n",
    "    def on_created(self, event):\n",
    "        if not event.is_directory:\n",
    "            self.update_vectorstore(event.src_path)\n",
    "\n",
    "    def update_vectorstore(self, file_path):\n",
    "        global update_notification\n",
    "        print(f\"Detected change in {file_path}. Updating vector store.\")\n",
    "        \n",
    "        # Delete existing documents from the vector store that match the source file\n",
    "        existing_docs = self.vectorstore.get(where={'source': file_path})\n",
    "        if existing_docs['ids']:\n",
    "            self.vectorstore.delete(ids=existing_docs['ids'])\n",
    "            print(f\"Deleted {len(existing_docs['ids'])} old chunks for {file_path}\")\n",
    "\n",
    "        # Load the new/modified document and add it to the vector store\n",
    "        docs = load_single_document(file_path)\n",
    "        if docs:\n",
    "            chunks = self.text_splitter.split_documents(docs)\n",
    "            self.vectorstore.add_documents(chunks)\n",
    "            print(f\"Added {len(chunks)} new chunks for {file_path}\")\n",
    "            update_notification = f\"Knowledge base updated with changes from: {os.path.basename(file_path)}\"\n",
    "\n",
    "def start_watcher(path, vectorstore, text_splitter):\n",
    "    event_handler = KnowledgeBaseWatcher(vectorstore, text_splitter)\n",
    "    observer = Observer()\n",
    "    observer.schedule(event_handler, path, recursive=True)\n",
    "    observer.start()\n",
    "    print(f\"Watching for changes in {path}\")\n",
    "    try:\n",
    "        while True:\n",
    "            time.sleep(1)\n",
    "    except KeyboardInterrupt:\n",
    "        observer.stop()\n",
    "    observer.join()\n",
    "\n",
    "# Start the file watcher in a background thread\n",
    "watcher_thread = threading.Thread(\n",
    "    target=start_watcher,\n",
    "    args=(\"knowledge_base\", vectorstore, text_splitter),\n",
    "    daemon=True\n",
    ")\n",
    "watcher_thread.start()\n",
    "\n",
    "def chat(message, history):\n",
    "    global update_notification\n",
    "    \n",
    "    # Check if there's an update notification to display\n",
    "    if update_notification:\n",
    "        notification_message = f\"âœ¨ *Knowledge base was recently updated. The answer below may reflect new information.*\\n\\n---\\n\"\n",
    "        update_notification = \"\" # Reset after displaying\n",
    "    else:\n",
    "        notification_message = \"\"\n",
    "\n",
    "    response = qa_chain.invoke({\"query\": message})\n",
    "    return notification_message + response[\"result\"]\n",
    "\n",
    "iface = gr.ChatInterface(fn=chat, title=\"Knowledge Base Chat\", chatbot=gr.Chatbot(height=600))\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f08c5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Error loading file knowledge_base/drive/ai_team_members.docx: Package not found at 'knowledge_base/drive/ai_team_members.docx'\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Error loading file knowledge_base/drive/ai_team_members.docx: Package not found at 'knowledge_base/drive/ai_team_members.docx'\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Error loading file knowledge_base/drive/ai_team_members.docx: Package not found at 'knowledge_base/drive/ai_team_members.docx'\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Error loading file knowledge_base/drive/ai_team_members.docx: Package not found at 'knowledge_base/drive/ai_team_members.docx'\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Error loading file knowledge_base/drive/ai_team_members.docx: Package not found at 'knowledge_base/drive/ai_team_members.docx'\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Error loading file knowledge_base/drive/ai_team_members.docx: Package not found at 'knowledge_base/drive/ai_team_members.docx'\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Error loading file knowledge_base/drive/ai_team_members.docx: Package not found at 'knowledge_base/drive/ai_team_members.docx'\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Error loading file knowledge_base/drive/ai_team_members.docx: Package not found at 'knowledge_base/drive/ai_team_members.docx'\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Error loading file knowledge_base/drive/ai_team_members.docx: Package not found at 'knowledge_base/drive/ai_team_members.docx'\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Error loading file knowledge_base/drive/ai_team_members.docx: Package not found at 'knowledge_base/drive/ai_team_members.docx'\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Error loading file knowledge_base/drive/ai_team_members.docx: Package not found at 'knowledge_base/drive/ai_team_members.docx'\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Error loading file knowledge_base/drive/ai_team_members.docx: Package not found at 'knowledge_base/drive/ai_team_members.docx'\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Error loading file knowledge_base/drive/ai_team_members.docx: Package not found at 'knowledge_base/drive/ai_team_members.docx'\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Error loading file knowledge_base/drive/ai_team_members.docx: Package not found at 'knowledge_base/drive/ai_team_members.docx'\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Error loading file knowledge_base/drive/ai_team_members.docx: Package not found at 'knowledge_base/drive/ai_team_members.docx'\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Error loading file knowledge_base/drive/ai_team_members.docx: Package not found at 'knowledge_base/drive/ai_team_members.docx'\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Error loading file knowledge_base/drive/ai_team_members.docx: Package not found at 'knowledge_base/drive/ai_team_members.docx'\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Error loading file knowledge_base/drive/ai_team_members.docx: Package not found at 'knowledge_base/drive/ai_team_members.docx'\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Error loading file knowledge_base/drive/ai_team_members.docx: Package not found at 'knowledge_base/drive/ai_team_members.docx'\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Error loading file knowledge_base/drive/ai_team_members.docx: Package not found at 'knowledge_base/drive/ai_team_members.docx'\n",
      "Error loading file knowledge_base/drive/ai_team_members.docx: Package not found at 'knowledge_base/drive/ai_team_members.docx'\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Error loading file knowledge_base/drive/ai_team_members.docx: Package not found at 'knowledge_base/drive/ai_team_members.docx'\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Error loading file knowledge_base/drive/ai_team_members.docx: Package not found at 'knowledge_base/drive/ai_team_members.docx'\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Error loading file knowledge_base/drive/ai_team_members.docx: Package not found at 'knowledge_base/drive/ai_team_members.docx'\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Error loading file knowledge_base/drive/ai_team_members.docx: Package not found at 'knowledge_base/drive/ai_team_members.docx'\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Error loading file knowledge_base/drive/ai_team_members.docx: Package not found at 'knowledge_base/drive/ai_team_members.docx'\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Error loading file knowledge_base/drive/ai_team_members.docx: Package not found at 'knowledge_base/drive/ai_team_members.docx'\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Error loading file knowledge_base/drive/ai_team_members.docx: Package not found at 'knowledge_base/drive/ai_team_members.docx'\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Created dummy docx file at: /home/thunder/projects/one_stop/knowledge_base/drive/ai_team_members.docx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 1 new chunks for knowledge_base/drive/ai_team_members.docxAdded 1 new chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Deleted 2 old chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Deleted 2 old chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Added 1 new chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Deleted 1 old chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Added 1 new chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Deleted 1 old chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Added 1 new chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Deleted 1 old chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Added 1 new chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Deleted 1 old chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Added 1 new chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Deleted 1 old chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Added 1 new chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Deleted 1 old chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Added 1 new chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Deleted 1 old chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Added 1 new chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Deleted 1 old chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Added 1 new chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Deleted 1 old chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Added 1 new chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Deleted 1 old chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Added 1 new chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Deleted 1 old chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Added 1 new chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Deleted 1 old chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Added 1 new chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Deleted 1 old chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Added 1 new chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Deleted 1 old chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Added 1 new chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Deleted 1 old chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Added 1 new chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Deleted 1 old chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Added 1 new chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Deleted 1 old chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Added 1 new chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Deleted 1 old chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Added 1 new chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Deleted 1 old chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Added 1 new chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Deleted 1 old chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Added 1 new chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Deleted 1 old chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Added 1 new chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Added 1 new chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Detected change in knowledge_base/drive/ai_team_members.docx. Updating vector store.\n",
      "Deleted 1 old chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Added 1 new chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Added 1 new chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Added 1 new chunks for knowledge_base/drive/ai_team_members.docx\n",
      "Detected change in knowledge_base/slack/slack_chat.txt. Updating vector store.\n",
      "Detected change in knowledge_base/slack/slack_chat.txt. Updating vector store.\n",
      "Deleted 1 old chunks for knowledge_base/slack/slack_chat.txt\n",
      "Deleted 1 old chunks for knowledge_base/slack/slack_chat.txt\n",
      "Detected change in knowledge_base/slack/slack_chat.txt. Updating vector store.\n",
      "Detected change in knowledge_base/slack/slack_chat.txt. Updating vector store.\n",
      "Deleted 1 old chunks for knowledge_base/slack/slack_chat.txt\n",
      "Deleted 1 old chunks for knowledge_base/slack/slack_chat.txt\n",
      "Added 1 new chunks for knowledge_base/slack/slack_chat.txt\n",
      "Added 1 new chunks for knowledge_base/slack/slack_chat.txt\n",
      "Added 1 new chunks for knowledge_base/slack/slack_chat.txt\n",
      "Added 1 new chunks for knowledge_base/slack/slack_chat.txt\n",
      "Detected change in knowledge_base/slack/slack_chat.txt. Updating vector store.\n",
      "Detected change in knowledge_base/slack/slack_chat.txt. Updating vector store.\n",
      "Deleted 2 old chunks for knowledge_base/slack/slack_chat.txt\n",
      "Detected change in knowledge_base/slack/slack_chat.txt. Updating vector store.\n",
      "Detected change in knowledge_base/slack/slack_chat.txt. Updating vector store.\n",
      "Deleted 2 old chunks for knowledge_base/slack/slack_chat.txt\n",
      "Added 1 new chunks for knowledge_base/slack/slack_chat.txt\n",
      "Detected change in knowledge_base/slack/slack_chat.txt. Updating vector store.\n",
      "Deleted 1 old chunks for knowledge_base/slack/slack_chat.txt\n",
      "Added 1 new chunks for knowledge_base/slack/slack_chat.txt\n",
      "Detected change in knowledge_base/slack/slack_chat.txt. Updating vector store.\n",
      "Deleted 1 old chunks for knowledge_base/slack/slack_chat.txt\n",
      "Added 1 new chunks for knowledge_base/slack/slack_chat.txt\n",
      "Detected change in knowledge_base/slack/slack_chat.txt. Updating vector store.\n",
      "Deleted 1 old chunks for knowledge_base/slack/slack_chat.txt\n",
      "Added 1 new chunks for knowledge_base/slack/slack_chat.txt\n",
      "Added 1 new chunks for knowledge_base/slack/slack_chat.txt\n",
      "Detected change in knowledge_base/slack/slack_chat.txt. Updating vector store.\n",
      "Deleted 1 old chunks for knowledge_base/slack/slack_chat.txt\n",
      "Added 1 new chunks for knowledge_base/slack/slack_chat.txt\n",
      "Added 1 new chunks for knowledge_base/slack/slack_chat.txt\n",
      "Added 1 new chunks for knowledge_base/slack/slack_chat.txt\n",
      "Detected change in knowledge_base/slack/slack_chat.txt. Updating vector store.\n",
      "Detected change in knowledge_base/slack/slack_chat.txt. Updating vector store.\n",
      "Deleted 2 old chunks for knowledge_base/slack/slack_chat.txt\n",
      "Deleted 2 old chunks for knowledge_base/slack/slack_chat.txt\n",
      "Detected change in knowledge_base/slack/slack_chat.txt. Updating vector store.\n",
      "Detected change in knowledge_base/slack/slack_chat.txt. Updating vector store.\n",
      "Deleted 2 old chunks for knowledge_base/slack/slack_chat.txt\n",
      "Deleted 2 old chunks for knowledge_base/slack/slack_chat.txt\n",
      "Added 1 new chunks for knowledge_base/slack/slack_chat.txt\n",
      "Added 1 new chunks for knowledge_base/slack/slack_chat.txt\n",
      "Detected change in knowledge_base/slack/slack_chat.txt. Updating vector store.\n",
      "Deleted 2 old chunks for knowledge_base/slack/slack_chat.txt\n",
      "Added 1 new chunks for knowledge_base/slack/slack_chat.txt\n",
      "Added 1 new chunks for knowledge_base/slack/slack_chat.txt\n",
      "Detected change in knowledge_base/slack/slack_chat.txt. Updating vector store.\n",
      "Deleted 2 old chunks for knowledge_base/slack/slack_chat.txt\n",
      "Added 1 new chunks for knowledge_base/slack/slack_chat.txt\n",
      "Added 1 new chunks for knowledge_base/slack/slack_chat.txt\n",
      "Detected change in knowledge_base/drive/new.txt. Updating vector store.\n",
      "Detected change in knowledge_base/drive/new.txt. Updating vector store.\n",
      "Detected change in knowledge_base/drive/new.txt. Updating vector store.\n",
      "Detected change in knowledge_base/drive/new.txt. Updating vector store.\n",
      "Added 1 new chunks for knowledge_base/drive/new.txt\n",
      "Added 1 new chunks for knowledge_base/drive/new.txt\n",
      "Added 1 new chunks for knowledge_base/drive/new.txt\n",
      "Added 1 new chunks for knowledge_base/drive/new.txt\n"
     ]
    }
   ],
   "source": [
    "import docx\n",
    "\n",
    "# Create a new Document\n",
    "doc = docx.Document()\n",
    "\n",
    "# Add a heading\n",
    "doc.add_heading('AI Team Members', level=1)\n",
    "\n",
    "# Add team member names\n",
    "team_members = [\n",
    "    \"Alice - Lead AI Scientist\",\n",
    "    \"Bob - Machine Learning Engineer\",\n",
    "    \"Charlie - Data Scientist\",\n",
    "    \"David - AI Ethicist\",\n",
    "    \"Eve - NLP Specialist\"\n",
    "]\n",
    "\n",
    "for member in team_members:\n",
    "    doc.add_paragraph(member, style='List Bullet')\n",
    "\n",
    "# Save the document\n",
    "file_path = \"/home/thunder/projects/one_stop/knowledge_base/drive/ai_team_members.docx\"\n",
    "doc.save(file_path)\n",
    "\n",
    "print(f\"Created dummy docx file at: {file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
